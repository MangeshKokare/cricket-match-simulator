{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9df4875-9701-45e0-896e-c349ffe8229d",
   "metadata": {},
   "source": [
    "# Problem Statement\n",
    "Create a cricket match simulator  where an AI agent decides the best batting strategy to maximize runs and chase a target within a given number of balls."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9bf9b2-d0d8-408a-9d0b-881e4c54ae28",
   "metadata": {},
   "source": [
    "# 1: Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce6890c4-36f4-4a89-9935-1abc3b84dd0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b56778f-e732-4393-b14e-f2a0c1d1818b",
   "metadata": {},
   "source": [
    "# 2: Define the Cricket Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e7a9043b-5f89-4f89-b8ff-c3229603f68d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MatchEnv:\n",
    "    def __init__(self):\n",
    "        self.max_balls = 6\n",
    "        self.max_runs = 20\n",
    "        self.action_space = 6  # 0 to 5 (e.g., defend, single, double, four, six, risky)\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.balls_left = self.max_balls\n",
    "        self.runs_needed = self.max_runs\n",
    "        self.done = False\n",
    "        return (self.balls_left, self.runs_needed)\n",
    "\n",
    "    def step(self, action):\n",
    "        action_rewards = {\n",
    "            0: 0,\n",
    "            1: 1,\n",
    "            2: 2,\n",
    "            3: 4,\n",
    "            4: 6,\n",
    "            5: 0\n",
    "        }\n",
    "\n",
    "        reward = 0\n",
    "        out = False\n",
    "\n",
    "        if action == 5:\n",
    "            if np.random.rand() < 0.5:\n",
    "                reward = -10\n",
    "                out = True\n",
    "            else:\n",
    "                reward = 6\n",
    "                self.runs_needed -= 6\n",
    "        else:\n",
    "            runs = action_rewards[action]\n",
    "            reward = runs\n",
    "            self.runs_needed -= runs\n",
    "\n",
    "        self.balls_left -= 1\n",
    "        self.runs_needed = max(0, self.runs_needed)\n",
    "\n",
    "        if self.balls_left == 0 or self.runs_needed == 0 or out:\n",
    "            self.done = True\n",
    "\n",
    "        return (self.balls_left, self.runs_needed), reward, self.done\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "613cc442-1ae3-4f27-854f-de914d3e5e74",
   "metadata": {},
   "source": [
    "# 3: Initialize Environment and Q-table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b2709ca-4a2a-4f6d-929a-f0717570917c",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = MatchEnv()\n",
    "q_table = np.zeros((7, 21, env.action_space))  # (balls, runs, actions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a4b049-142b-431b-adc3-3fcf8b75d7da",
   "metadata": {},
   "source": [
    "# 4: Define Training Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f7f27760-c1e9-4aef-bee3-1a167e986290",
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes = 10000\n",
    "learning_rate = 0.1\n",
    "discount_factor = 0.95\n",
    "epsilon = 1.0\n",
    "epsilon_decay = 0.995\n",
    "min_epsilon = 0.01\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb68c5f3-f277-4727-9457-a87e2ed35738",
   "metadata": {},
   "source": [
    "# 5: Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74244ac2-fbb2-413a-9e3c-52da80d6968e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for episode in range(episodes):\n",
    "    state = env.reset()\n",
    "\n",
    "    while not env.done:\n",
    "        balls, runs = state\n",
    "\n",
    "        # Exploration vs Exploitation\n",
    "        if np.random.rand() < epsilon:\n",
    "            action = np.random.randint(env.action_space)\n",
    "        else:\n",
    "            action = np.argmax(q_table[balls, runs])\n",
    "\n",
    "        next_state, reward, done = env.step(action)\n",
    "        b, r = next_state\n",
    "\n",
    "        \n",
    "        q_table[balls, runs, action] = q_table[balls, runs, action] + learning_rate * (\n",
    "            reward + discount_factor * np.max(q_table[b, r]) - q_table[balls, runs, action]\n",
    "        )\n",
    "\n",
    "        state = next_state\n",
    "\n",
    "    # Decay epsilon\n",
    "    epsilon = max(min_epsilon, epsilon * epsilon_decay)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ca1906-1f6c-4ab2-898a-4815bfadbf27",
   "metadata": {},
   "source": [
    "# 6: Test the Trained Agent (with User Input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9e6bee52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üß† Agent's Decisions:\n",
      "\n",
      "Balls Left: 4, Runs Needed: 16, Action: Six, Reward: 6\n",
      "Balls Left: 3, Runs Needed: 10, Action: Boundary (4), Reward: 4\n",
      "Balls Left: 2, Runs Needed: 6, Action: Six, Reward: 6\n",
      "\n",
      "‚úÖ Target Achieved!\n"
     ]
    }
   ],
   "source": [
    "user_balls_left = int(input(\"Enter balls left (0 to 6): \"))\n",
    "user_runs_needed = int(input(\"Enter runs needed (0 to 20): \"))\n",
    "\n",
    "if user_balls_left < 0 or user_balls_left > 6 or user_runs_needed < 0 or user_runs_needed > 20:\n",
    "    print(\"Invalid input.\")\n",
    "else:\n",
    "    state = (user_balls_left, user_runs_needed)\n",
    "    env.balls_left = user_balls_left\n",
    "    env.runs_needed = user_runs_needed\n",
    "    env.done = False\n",
    "\n",
    "    action_map = {\n",
    "        0: \"Defend\",\n",
    "        1: \"Single\",\n",
    "        2: \"Double\",\n",
    "        3: \"Boundary (4)\",\n",
    "        4: \"Six\",\n",
    "        5: \"Risky Shot\"\n",
    "    }\n",
    "\n",
    "    print(\"\\nüß† Agent's Decisions:\\n\")\n",
    "    while not env.done:\n",
    "        balls, runs = state\n",
    "        action = np.argmax(q_table[balls, runs])\n",
    "        next_state, reward, done = env.step(action)\n",
    "\n",
    "        print(f\"Balls Left: {balls}, Runs Needed: {runs}, \"\n",
    "              f\"Action: {action_map[action]}, Reward: {reward}\")\n",
    "\n",
    "        state = next_state\n",
    "\n",
    "    if env.runs_needed == 0:\n",
    "        print(\"\\n‚úÖ Target Achieved!\")\n",
    "    else:\n",
    "        print(\"\\n‚ùå Target Not Achieved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "481e5f52-f2c2-4020-b4ae-8815f655ceed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Assuming your Q-table is stored in a variable named q_table\n",
    "with open(\"q_table.pkl\", \"wb\") as f:\n",
    "    pickle.dump(q_table, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e46988d1-35e9-4d5d-b61a-17cd5f1805cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
